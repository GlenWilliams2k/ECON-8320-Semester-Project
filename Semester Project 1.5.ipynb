{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Getting the data into Python, and cleaning it.\n",
    "- will need to write code to import and clean, then functionalize it.\n",
    "\n",
    "Steps to clean data:\n",
    "\n",
    "*Remaining balance:\n",
    "- need to remove $ and ,\n",
    "- convert to integer\n",
    "pandas already did it\n",
    "\n",
    "*location:\n",
    "city names contain misspellings and characters\n",
    "state names are not all abbreviated\n",
    "some zip codes have postal codes\n",
    "- start by assigning all blank values as missing  \n",
    "- pull only the first part of the zip into a new col\n",
    "- if original zip col is not missing look zip up in zippopotamus to return city and state, \n",
    "- if original zip is missing or blank, look up city and state\n",
    "- if city and state are missing, return missing\n",
    "\n",
    "Language:\n",
    "-correct language blanks to missing\n",
    "\n",
    "*DOB:\n",
    "-assign DOBs before today as NA\n",
    "\n",
    "Marital status:\n",
    "- assign blanks to missing\n",
    "\n",
    "*Gender:\n",
    "- assign blanks to missing\n",
    "\n",
    "*Race:\n",
    "- some values for white misspelled\n",
    "- some values for American Indian misspelled\n",
    "- if contains american indian, then American Indian or Alaska Native\n",
    " - if starts with W, then white\n",
    "- assign blanks to missing\n",
    "\n",
    "*Hispanic/Latino:\n",
    "- some values mispelled\n",
    "- non-hispanic or latino not consistent\n",
    "- some values no\n",
    "- assign blanks to missing\n",
    "- Everything that starts with no should be assigned to non-hispanic or latino\n",
    "- if doesn't start with no or is missing, decline to answer,  or non-hispanic, assign to Hispanic or Latino\n",
    "\n",
    "*Sexual orientation:\n",
    "- assign blanks and N/As to missing\n",
    "- assign decline to \"decline to answer\"\n",
    "- If it starts with st assign to straight\n",
    "\n",
    "*Insurance type:\n",
    "- if contains medicare or medicaid, assign to Medicare & Medicaid\n",
    "- if starts with un then uninsured\n",
    "- if missing or blank, assign missing\n",
    "\n",
    "Household size:\n",
    "- assign the row with 4602 to blank\n",
    "- assign missings to blank?\n",
    "\n",
    "*Household income:\n",
    "- remove $ - and , assign as integers\n",
    "- assign missings to blank\n",
    "pandas already did it\n",
    "\n",
    "*Distance round trip:\n",
    "- take only numbers, assign text to missing\n",
    "\n",
    "referral source:\n",
    "- assign blanks to missing\n",
    "\n",
    "*Amount:\n",
    " - take only numbers, remove $ - and ,\n",
    "- assign blanks to missing\n",
    "\n",
    "*payment method:\n",
    "only text, assign responses with only numbers to missing\n",
    "\n",
    "payable to:\n",
    "Surely I don't have to do anything with this\n",
    "\n",
    "*patient letter notified:\n",
    "- assign na, n/a, missing, and blanks to No\n",
    "- assign dates to Y\n",
    "\n",
    "Application signed:\n",
    "should be fine.\n",
    "\n",
    "other:\n",
    "make sure data types align with what's needed\n",
    "- use type \"object\" to handle numerical and non-numerical data?\n",
    "Bulleted cols need to be cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do:\n",
    "-cleaning for address using zipopotamus api\n",
    "-lesson 9 to guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#function to fetch zip codes\n",
    "def fetch_zip_info(zip_codes):\n",
    "    \"\"\"Fetch city and state info for zip codes.\"\"\"\n",
    "    zip_to_locale = {}\n",
    "    for zip_code in zip_codes:\n",
    "        try:\n",
    "            url = f\"https://api.zippopotam.us/us/{zip_code}\"\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                zip_data = response.json()\n",
    "                city = zip_data['places'][0]['place name']\n",
    "                state = zip_data['places'][0]['state abbreviation']\n",
    "                zip_to_locale[zip_code] = {'City': city, 'State': state}\n",
    "            else:\n",
    "                zip_to_locale[zip_code] = {'City': 'Unknown', 'State': 'Unknown'}\n",
    "        except Exception:\n",
    "            zip_to_locale[zip_code] = {'City': 'Error', 'State': 'Error'}\n",
    "    return zip_to_locale\n",
    "\n",
    "def clean_data(filepath, sheet_name=\"None\"):\n",
    "    \"\"\"Clean the service learning data.\"\"\"\n",
    "    today = pd.Timestamp.today()\n",
    "\n",
    "    # Read file (Excel or CSV)\n",
    "    if filepath.endswith('.xlsx'):\n",
    "        data = pd.read_excel(filepath, sheet_name=sheet_name)\n",
    "    elif filepath.endswith('.csv'):\n",
    "        data = pd.read_csv(filepath)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: Only .csv or .xlsx allowed.\")\n",
    "\n",
    "    # Clean Payment Submitted Date\n",
    "    def extract_date(text):\n",
    "        if isinstance(text, pd.Timestamp):\n",
    "            return text.strftime(\"%m/%d/%Y\")\n",
    "        match = re.search(r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\\b', str(text))\n",
    "        return match.group(0) if match else None\n",
    "    # Extract dates\n",
    "    data['Payment Submitted Date'] = data['Payment Submitted?'].apply(extract_date)\n",
    "    # Convert to datetime\n",
    "    data['Payment Submitted Date'] = pd.to_datetime(data['Payment Submitted Date'], errors='coerce')\n",
    "    # Ensure the column is explicitly set to 'Yes' if a valid date was found\n",
    "    has_date = data['Payment Submitted Date'].notna()\n",
    "    data.loc[has_date, 'Payment Submitted?'] = 'Yes'\n",
    "\n",
    "    # Clean Zip, City, State\n",
    "    data['Pt Zip'] = data['Pt Zip'].astype(str).str.strip().str.extract(r'(\\d{5})')[0]\n",
    "    data['Pt Zip'] = data['Pt Zip'].fillna(\"Missing\")\n",
    "\n",
    "    data.loc[data['Pt Zip'] == \"Missing\", ['Pt City', 'Pt State']] = \"Missing\"\n",
    "\n",
    "    valid_zips = data[data['Pt Zip'] != \"Missing\"]['Pt Zip'].unique()\n",
    "    zip_to_locale = fetch_zip_info(valid_zips)\n",
    "\n",
    "    data['Pt City'] = data['Pt Zip'].apply(lambda z: zip_to_locale.get(z, {}).get('City', 'Missing'))\n",
    "    data['Pt State'] = data['Pt Zip'].apply(lambda z: zip_to_locale.get(z, {}).get('State', 'Missing'))\n",
    "\n",
    "    # Clean DOB\n",
    "    data['DOB'] = pd.to_datetime(data['DOB'], errors='coerce')\n",
    "    data.loc[data['DOB'] > today, 'DOB'] = pd.NaT\n",
    " \n",
    "    # Clean Gender\n",
    "    data['Gender'] = data['Gender'].replace(r'^\\s*$', \"Missing\", regex=True)\n",
    "\n",
    "    # Clean Race\n",
    "    data['Race'] = data['Race'].astype(str).str.strip().str.lower()\n",
    "    data['Race'] = data['Race'].apply(lambda x: (\n",
    "        'American Indian or Alaska Native' if 'american indian' in x else\n",
    "        'White' if x.startswith('w') else\n",
    "        \"Missing\" if x in ['', 'nan'] else x.title()\n",
    "    ))\n",
    "\n",
    "    # Clean Hispanic/Latino\n",
    "    data['Hispanic/Latino'] = data['Hispanic/Latino'].astype(str).str.strip().str.lower()\n",
    "    data['Hispanic/Latino'] = data['Hispanic/Latino'].apply(lambda x: (\n",
    "        'Non-Hispanic or Latino' if x.startswith('no') else\n",
    "        'Hispanic or Latino' if not (x.startswith('no') or x in ['nan', '', 'missing', 'decline to answer', 'non-hispanic']) else\n",
    "        np.nan\n",
    "    ))\n",
    "\n",
    "    # Clean Sexual Orientation\n",
    "    data['Sexual Orientation'] = data['Sexual Orientation'].astype(str).str.strip().str.lower()\n",
    "    data['Sexual Orientation'] = data['Sexual Orientation'].apply(lambda x: (\n",
    "        'Decline to answer' if x == 'decline' else\n",
    "        'Straight' if x.startswith('st') else\n",
    "        np.nan if x in ['n/a', '', 'nan'] else x.title()\n",
    "    ))\n",
    "\n",
    "    # Clean Insurance Type\n",
    "    data['Insurance Type'] = data['Insurance Type'].astype(str).str.strip().str.lower()\n",
    "    data['Insurance Type'] = data['Insurance Type'].apply(lambda x: (\n",
    "        'Medicare & Medicaid' if 'medicare' in x or 'medicaid' in x else\n",
    "        'Uninsured' if x.startswith('un') else\n",
    "        'Missing' if x in ['', 'nan'] else\n",
    "        x.title()\n",
    "    ))\n",
    "\n",
    "    # Marital Status, Gender, Hispanic/Latino, Sexual Orientation blanks to \"Missing\"\n",
    "    for col in ['Marital Status', 'Gender', 'Hispanic/Latino', 'Sexual Orientation']:\n",
    "        data[col] = data[col].astype(str).str.strip().replace(r'^\\s*$', 'Missing', regex=True).replace('nan', 'Missing')\n",
    "\n",
    "    # Sexual Orientation further normalization\n",
    "    data['Sexual Orientation'] = data['Sexual Orientation'].str.lower().apply(lambda x: (\n",
    "        'Decline to answer' if x == 'decline' else\n",
    "        'Straight' if x.startswith('st') else\n",
    "        x.title()\n",
    "    ))\n",
    "\n",
    "    # Clean HouseHold Size\n",
    "    data['Household Size'] = pd.to_numeric(data['Household Size'], errors='coerce')\n",
    "    data.loc[(data['Household Size'] > 20) | (data['Household Size'].isna()), 'Household Size'] = np.nan\n",
    "\n",
    "    # Clean Total Household Gross Monthly Income\n",
    "    data['Total Household Gross Monthly Income'] = (\n",
    "        data['Total Household Gross Monthly Income']\n",
    "        .astype(str).str.replace(r'[^\\d.]', '', regex=True)\n",
    "    )\n",
    "    data['Total Household Gross Monthly Income'] = pd.to_numeric(data['Total Household Gross Monthly Income'], errors='coerce')\n",
    "\n",
    "    # Clean Distance roundtrip\n",
    "    data['Distance roundtrip/Tx'] = pd.to_numeric(\n",
    "        data['Distance roundtrip/Tx'].astype(str).str.extract(r'(\\d+\\.?\\d*)')[0],\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Clean Referral Source\n",
    "    data['Referral Source'] = data['Referral Source'].astype(str).str.strip().replace(r'^\\s*$', 'Missing', regex=True)\n",
    "\n",
    "    # Clean Payment Method\n",
    "    data['Payment Method'] = data['Payment Method'].astype(str).str.strip().replace(r'^\\s*$', 'Missing', regex=True).replace('nan', 'Missing')\n",
    "     # Strip everything except letters\n",
    "    data['Payment Method'] = data['Payment Method'].str.replace(r'[^a-zA-Z\\s]','', regex=True).str.strip()\n",
    "    #Uppercase\n",
    "    data['Payment Method'] = data['Payment Method'].str.upper()\n",
    "\n",
    "    # Clean Remaining Balance\n",
    "    data['Remaining Balance'] = pd.to_numeric(data['Remaining Balance'], errors='coerce').round(2)\n",
    "\n",
    "    # Clean Patient Letter Notified\n",
    "    def letter_notified(val):\n",
    "        val = str(val).strip().lower()\n",
    "        if val in ['na', 'n/a', 'missing', '', 'nan']:\n",
    "            return 'No'\n",
    "        try:\n",
    "            pd.to_datetime(val)\n",
    "            return 'Yes'\n",
    "        except:\n",
    "            return 'No'\n",
    "    \n",
    "    data['Patient Letter Notified? (Directly/Indirectly through rep)'] = data['Patient Letter Notified? (Directly/Indirectly through rep)'].apply(letter_notified)\n",
    "       \n",
    "    #Clean Payable to:\n",
    "    # Replace blanks or whitespace-only strings with \"Missing\"\n",
    "    data['Payable to:'] = data['Payable to:'].replace(r'^\\s*$', \"Missing\", regex=True)\n",
    "\n",
    "    # Export cleaned data for testing\n",
    "    #output_path = r\"C:\\Users\\Glen\\Documents\\Tools For Data Analysis\\Semester Project\\cleaned_data.csv\"\n",
    "    #data.to_csv(output_path, index=False)\n",
    "\n",
    "    #Export cleaned data as part of github action\n",
    "    output_path = os.path.join(\"output\", \"cleaned_data.csv\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    data.to_csv(output_path, index=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-18>:66: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID#</th>\n",
       "      <th>Grant Req Date</th>\n",
       "      <th>App Year</th>\n",
       "      <th>Remaining Balance</th>\n",
       "      <th>Request Status</th>\n",
       "      <th>Payment Submitted?</th>\n",
       "      <th>Reason - Pending/No</th>\n",
       "      <th>Pt City</th>\n",
       "      <th>Pt State</th>\n",
       "      <th>Pt Zip</th>\n",
       "      <th>Language</th>\n",
       "      <th>DOB</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Hispanic/Latino</th>\n",
       "      <th>Sexual Orientation</th>\n",
       "      <th>Insurance Type</th>\n",
       "      <th>Household Size</th>\n",
       "      <th>Total Household Gross Monthly Income</th>\n",
       "      <th>Distance roundtrip/Tx</th>\n",
       "      <th>Referral Source</th>\n",
       "      <th>Referred By:</th>\n",
       "      <th>Type of Assistance (CLASS)</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Payable to:</th>\n",
       "      <th>Patient Letter Notified? (Directly/Indirectly through rep)</th>\n",
       "      <th>Application Signed?</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Payment Submitted Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180001</td>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>1</td>\n",
       "      <td>1180.00</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>Dr. Natarajan/Lily Salinas</td>\n",
       "      <td>Medical Supplies/Prescription Co-pay(s)</td>\n",
       "      <td>320</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>190001</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1428.39</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>Pam Owen/Sheri Shannon\\n</td>\n",
       "      <td>Medical Supplies/Prescription Co-pay(s)</td>\n",
       "      <td>21.61</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190001</td>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>1</td>\n",
       "      <td>1428.39</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>Teresa Pfister</td>\n",
       "      <td>Food/Groceries</td>\n",
       "      <td>50</td>\n",
       "      <td>GC</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190002</td>\n",
       "      <td>2019-05-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1400.00</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>AG/Susan Keith</td>\n",
       "      <td>Food/Groceries</td>\n",
       "      <td>100</td>\n",
       "      <td>GC</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190003</td>\n",
       "      <td>2019-05-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1425.00</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>AG/Kristi McHugh</td>\n",
       "      <td>Medical Supplies/Prescription Co-pay(s)</td>\n",
       "      <td>75</td>\n",
       "      <td>CC</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>240393</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>2</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>No</td>\n",
       "      <td>HS</td>\n",
       "      <td>Falls City</td>\n",
       "      <td>NE</td>\n",
       "      <td>68355</td>\n",
       "      <td>English</td>\n",
       "      <td>1960-09-23</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>CPN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gas</td>\n",
       "      <td>500</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Waiting on HS</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>240393</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>2</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>No</td>\n",
       "      <td>HS</td>\n",
       "      <td>Falls City</td>\n",
       "      <td>NE</td>\n",
       "      <td>68355</td>\n",
       "      <td>English</td>\n",
       "      <td>1960-09-23</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>CPN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Food/Groceries</td>\n",
       "      <td>500</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Waiting on HS</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>240548</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>2</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>NE</td>\n",
       "      <td>68025</td>\n",
       "      <td>English</td>\n",
       "      <td>1962-04-03</td>\n",
       "      <td>Married</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Private</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2895.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NCS</td>\n",
       "      <td>ALISA SEIDLER</td>\n",
       "      <td>Multiple</td>\n",
       "      <td>1068.56</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>250038</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hastings</td>\n",
       "      <td>NE</td>\n",
       "      <td>68901</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1980-10-02</td>\n",
       "      <td>Single</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>2.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Morrison Cancer Center</td>\n",
       "      <td>Kellie Sterkel-SW</td>\n",
       "      <td>Housing</td>\n",
       "      <td>1500</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>250040</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hastings</td>\n",
       "      <td>NE</td>\n",
       "      <td>68901</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1980-10-02</td>\n",
       "      <td>Single</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>2.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Morrison Cancer Center</td>\n",
       "      <td>Kellie Sterkel</td>\n",
       "      <td>Housing</td>\n",
       "      <td>1500</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2292 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Patient ID# Grant Req Date  App Year  ...  Application Signed?           Notes Payment Submitted Date\n",
       "0          180001     2018-10-17         1  ...              Missing             NaN                    NaT\n",
       "1          190001     2019-01-03         1  ...              Missing             NaN                    NaT\n",
       "2          190001     2019-03-11         1  ...              Missing             NaN                    NaT\n",
       "3          190002     2019-05-20         1  ...              Missing             NaN                    NaT\n",
       "4          190003     2019-05-22         1  ...              Missing             NaN                    NaT\n",
       "...           ...            ...       ...  ...                  ...             ...                    ...\n",
       "2287       240393     2025-01-31         2  ...                  NaN  Waiting on HS                     NaT\n",
       "2288       240393     2025-01-31         2  ...                  NaN  Waiting on HS                     NaT\n",
       "2289       240548     2025-01-31         2  ...                  NaN             NaN                    NaT\n",
       "2290       250038     2025-01-31         1  ...                  NaN             NaN                    NaT\n",
       "2291       250040     2025-01-31         1  ...                  NaN             NaN                    NaT\n",
       "\n",
       "[2292 rows x 31 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing function\n",
    "#clean_data(\"C:\\\\Users\\\\Glen\\\\Documents\\\\Tools For Data Analysis\\\\Semester Project\\\\UNO Service Learning Data Sheet De-Identified Version.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        raise ValueError(\"❌ No input file provided. Usage: python clean_data_script.py <input_file>\")\n",
    "\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = os.path.splitext(input_file)[0] + \"_CLEANED.csv\"\n",
    "    sheet_name = \"PA Log Sheet\" if input_file.endswith(\".xlsx\") else None\n",
    "\n",
    "    cleaned_df = clean_data(input_file, sheet_name=sheet_name)\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Cleaned {input_file} -> {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub action steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name: Clean New Data\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - '**/*.csv'\n",
    "      - '**/*.xlsx'\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "jobs:\n",
    "  clean-data:\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "    - name: Checkout code\n",
    "      uses: actions/checkout@v4\n",
    "\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v5\n",
    "      with:\n",
    "        python-version: '3.x'\n",
    "\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install pandas numpy openpyxl\n",
    "\n",
    "    - name: Detect changed file\n",
    "      id: detect_file\n",
    "      run: |\n",
    "        CHANGED_FILE=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep -E '\\.csv$|\\.xlsx$' | head -n 1)\n",
    "        echo \"CHANGED_FILE=$CHANGED_FILE\" >> $GITHUB_ENV\n",
    "\n",
    "    - name: Run data cleaning script\n",
    "      if: env.CHANGED_FILE != ''\n",
    "      run: |\n",
    "        python clean_data_script.py \"${{ env.CHANGED_FILE }}\"\n",
    "\n",
    "    - name: Commit cleaned data\n",
    "      if: env.CHANGED_FILE != ''\n",
    "      run: |\n",
    "        git config --global user.name 'github-actions[bot]'\n",
    "        git config --global user.email 'github-actions[bot]@users.noreply.github.com'\n",
    "        git add *_CLEANED.csv\n",
    "        git commit -m \"Automated: Cleaned ${{ env.CHANGED_FILE }}\" || echo \"No changes to commit\"\n",
    "        git push\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dashboard and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title = \"Pending Applications\"\n",
    "    layout = \"wide\"\n",
    "    initial sidebar_state = 'expanded'\n",
    ")\n",
    "\n",
    "db_data = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
