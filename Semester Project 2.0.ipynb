{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Getting the data into Python, and cleaning it.\n",
    "- will need to write code to import and clean, then functionalize it.\n",
    "\n",
    "Steps to clean data:\n",
    "\n",
    "*Remaining balance:\n",
    "- need to remove $ and ,\n",
    "- convert to integer\n",
    "pandas already did it\n",
    "\n",
    "*location:\n",
    "city names contain misspellings and characters\n",
    "state names are not all abbreviated\n",
    "some zip codes have postal codes\n",
    "- start by assigning all blank values as missing  \n",
    "- pull only the first part of the zip into a new col\n",
    "- if original zip col is not missing look zip up in zippopotamus to return city and state, \n",
    "- if original zip is missing or blank, look up city and state\n",
    "- if city and state are missing, return missing\n",
    "\n",
    "Language:\n",
    "-correct language blanks to missing\n",
    "\n",
    "*DOB:\n",
    "-assign DOBs before today as NA\n",
    "\n",
    "Marital status:\n",
    "- assign blanks to missing\n",
    "\n",
    "*Gender:\n",
    "- assign blanks to missing\n",
    "\n",
    "*Race:\n",
    "- some values for white misspelled\n",
    "- some values for American Indian misspelled\n",
    "- if contains american indian, then American Indian or Alaska Native\n",
    " - if starts with W, then white\n",
    "- assign blanks to missing\n",
    "\n",
    "*Hispanic/Latino:\n",
    "- some values mispelled\n",
    "- non-hispanic or latino not consistent\n",
    "- some values no\n",
    "- assign blanks to missing\n",
    "- Everything that starts with no should be assigned to non-hispanic or latino\n",
    "- if doesn't start with no or is missing, decline to answer,  or non-hispanic, assign to Hispanic or Latino\n",
    "\n",
    "*Sexual orientation:\n",
    "- assign blanks and N/As to missing\n",
    "- assign decline to \"decline to answer\"\n",
    "- If it starts with st assign to straight\n",
    "\n",
    "*Insurance type:\n",
    "- if contains medicare or medicaid, assign to Medicare & Medicaid\n",
    "- if starts with un then uninsured\n",
    "- if missing or blank, assign missing\n",
    "\n",
    "Household size:\n",
    "- assign the row with 4602 to blank\n",
    "- assign missings to blank?\n",
    "\n",
    "*Household income:\n",
    "- remove $ - and , assign as integers\n",
    "- assign missings to blank\n",
    "pandas already did it\n",
    "\n",
    "*Distance round trip:\n",
    "- take only numbers, assign text to missing\n",
    "\n",
    "referral source:\n",
    "- assign blanks to missing\n",
    "\n",
    "*Amount:\n",
    " - take only numbers, remove $ - and ,\n",
    "- assign blanks to missing\n",
    "\n",
    "*payment method:\n",
    "only text, assign responses with only numbers to missing\n",
    "\n",
    "payable to:\n",
    "Surely I don't have to do anything with this\n",
    "\n",
    "*patient letter notified:\n",
    "- assign na, n/a, missing, and blanks to No\n",
    "- assign dates to Y\n",
    "\n",
    "Application signed:\n",
    "should be fine.\n",
    "\n",
    "other:\n",
    "make sure data types align with what's needed\n",
    "- use type \"object\" to handle numerical and non-numerical data?\n",
    "Bulleted cols need to be cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#function to fetch zip codes\n",
    "def fetch_zip_info(zip_codes):\n",
    "    \"\"\"Fetch city, state, latitude, and longitude for zip codes.\"\"\"\n",
    "    zip_to_locale = {}\n",
    "    for zip_code in zip_codes:\n",
    "        try:\n",
    "            url = f\"https://api.zippopotam.us/us/{zip_code}\"\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                zip_data = response.json()\n",
    "                place = zip_data['places'][0]\n",
    "                city = place['place name']\n",
    "                state = place['state abbreviation']\n",
    "                latitude = float(place['latitude'])\n",
    "                longitude = float(place['longitude'])\n",
    "                zip_to_locale[zip_code] = {\n",
    "                    'City': city,\n",
    "                    'State': state,\n",
    "                    'Latitude': latitude,\n",
    "                    'Longitude': longitude\n",
    "                }\n",
    "            else:\n",
    "                zip_to_locale[zip_code] = {\n",
    "                    'City': 'Unknown',\n",
    "                    'State': 'Unknown',\n",
    "                    'Latitude': None,\n",
    "                    'Longitude': None\n",
    "                }\n",
    "        except Exception:\n",
    "            zip_to_locale[zip_code] = {\n",
    "                'City': 'Error',\n",
    "                'State': 'Error',\n",
    "                'Latitude': None,\n",
    "                'Longitude': None\n",
    "            }\n",
    "    return zip_to_locale\n",
    "\n",
    "def clean_data(filepath, sheet_name = None):\n",
    "    \"\"\"Clean the service learning data.\"\"\"\n",
    "    today = pd.Timestamp.today()\n",
    "\n",
    "    # Read file (Excel or CSV)\n",
    "    if filepath.endswith('.xlsx'):\n",
    "        # Safely read Excel with specified sheet name\n",
    "        xl = pd.ExcelFile(filepath)\n",
    "        if sheet_name and sheet_name not in xl.sheet_names:\n",
    "            raise ValueError(f\"Sheet '{sheet_name}' not found. Available sheets: {xl.sheet_names}\")\n",
    "        data = xl.parse(sheet_name) if sheet_name else xl.parse(xl.sheet_names[0])\n",
    "    elif filepath.endswith('.csv'):\n",
    "        data = pd.read_csv(filepath)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: Only .csv or .xlsx allowed.\")\n",
    "\n",
    "    # Clean Payment Submitted Date\n",
    "    # Duplicate Payment Submitted?\n",
    "    data['Payment Submitted Date'] = data['Payment Submitted?']\n",
    "    # Convert to datetime and fill non-dates with NaT\n",
    "    data['Payment Submitted Date'] = pd.to_datetime(data['Payment Submitted Date'], errors='coerce')\n",
    "    # Ensure the column is explicitly set to 'Yes' if a valid date was found\n",
    "    data['Payment Submitted?'] = data['Payment Submitted?'].apply(\n",
    "    lambda x: x if pd.to_datetime(x, errors='coerce') is pd.NaT else 'Yes')\n",
    "\n",
    "    #Grant Request Date\n",
    "    data[\"Grant Req Date\"] = pd.to_datetime(data[\"Grant Req Date\"]).dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    # Clean Zip, City, State\n",
    "    data['Pt Zip'] = data['Pt Zip'].astype(str).str.strip().str.extract(r'(\\d{5})')[0]\n",
    "    data['Pt Zip'] = data['Pt Zip'].fillna(\"Missing\")\n",
    "    data.loc[data['Pt Zip'] == \"Missing\", ['Pt City', 'Pt State']] = \"Missing\"\n",
    "\n",
    "    valid_zips = data[data['Pt Zip'] != \"Missing\"]['Pt Zip'].unique()\n",
    "    zip_to_locale = fetch_zip_info(valid_zips)\n",
    "\n",
    "    # Add City, State, Latitude, Longitude\n",
    "    data['Pt City'] = data['Pt Zip'].apply(lambda z: zip_to_locale.get(z, {}).get('City', 'Missing'))\n",
    "    data['Pt State'] = data['Pt Zip'].apply(lambda z: zip_to_locale.get(z, {}).get('State', 'Missing'))\n",
    "    data['Latitude'] = data['Pt Zip'].apply(lambda z: zip_to_locale.get(z, {}).get('Latitude'))\n",
    "    data['Longitude'] = data['Pt Zip'].apply(lambda z: zip_to_locale.get(z, {}).get('Longitude'))\n",
    "\n",
    "    # Clean DOB\n",
    "    data['DOB'] = pd.to_datetime(data['DOB'], errors='coerce')\n",
    "    data.loc[data['DOB'] > today, 'DOB'] = pd.NaT\n",
    " \n",
    "    # Clean Gender\n",
    "    data['Gender'] = data['Gender'].replace(r'^\\s*$', \"Missing\", regex=True)\n",
    "\n",
    "    # Clean Race\n",
    "    data['Race'] = data['Race'].astype(str).str.strip().str.lower()\n",
    "    data['Race'] = data['Race'].apply(lambda x: (\n",
    "        'American Indian or Alaska Native' if 'american indian' in x else\n",
    "        'White' if x.startswith('w') else\n",
    "        \"Missing\" if x in ['', 'nan'] else x.title()\n",
    "    ))\n",
    "\n",
    "    # Clean Hispanic/Latino\n",
    "    data['Hispanic/Latino'] = data['Hispanic/Latino'].astype(str).str.strip().str.lower()\n",
    "    data['Hispanic/Latino'] = data['Hispanic/Latino'].apply(lambda x: (\n",
    "        'Non-Hispanic or Latino' if x.startswith('no') else\n",
    "        'Hispanic or Latino' if not (x.startswith('no') or x in ['nan', '', 'missing', 'decline to answer', 'non-hispanic']) else\n",
    "        np.nan\n",
    "    ))\n",
    "\n",
    "    # Clean Sexual Orientation\n",
    "    data['Sexual Orientation'] = data['Sexual Orientation'].astype(str).str.strip().str.lower()\n",
    "    data['Sexual Orientation'] = data['Sexual Orientation'].apply(lambda x: (\n",
    "        'Decline to answer' if x == 'decline' else\n",
    "        'Straight' if x.startswith('st') else\n",
    "        np.nan if x in ['n/a', '', 'nan'] else x.title()\n",
    "    ))\n",
    "\n",
    "    # Clean Insurance Type\n",
    "    data['Insurance Type'] = data['Insurance Type'].astype(str).str.strip().str.lower()\n",
    "    data['Insurance Type'] = data['Insurance Type'].apply(lambda x: (\n",
    "        'Medicare & Medicaid' if 'medicare' in x or 'medicaid' in x else\n",
    "        'Uninsured' if x.startswith('un') else\n",
    "        'Missing' if x in ['', 'nan'] else\n",
    "        x.title()\n",
    "    ))\n",
    "\n",
    "    # Marital Status, Gender, Hispanic/Latino, Sexual Orientation blanks to \"Missing\"\n",
    "    for col in ['Marital Status', 'Gender', 'Hispanic/Latino', 'Sexual Orientation']:\n",
    "        data[col] = data[col].astype(str).str.strip().replace(r'^\\s*$', 'Missing', regex=True).replace('nan', 'Missing')\n",
    "\n",
    "    # Sexual Orientation further normalization\n",
    "    data['Sexual Orientation'] = data['Sexual Orientation'].str.lower().apply(lambda x: (\n",
    "        'Decline to answer' if x == 'decline' else\n",
    "        'Straight' if x.startswith('st') else\n",
    "        x.title()\n",
    "    ))\n",
    "\n",
    "    # Clean HouseHold Size\n",
    "    data['Household Size'] = pd.to_numeric(data['Household Size'], errors='coerce')\n",
    "    data.loc[(data['Household Size'] > 20) | (data['Household Size'].isna()), 'Household Size'] = np.nan\n",
    "\n",
    "    # Clean Total Household Gross Monthly Income\n",
    "    data['Total Household Gross Monthly Income'] = (\n",
    "        data['Total Household Gross Monthly Income']\n",
    "        .astype(str).str.replace(r'[^\\d.]', '', regex=True)\n",
    "    )\n",
    "    data['Total Household Gross Monthly Income'] = pd.to_numeric(data['Total Household Gross Monthly Income'], errors='coerce')\n",
    "\n",
    "    # Clean Distance roundtrip\n",
    "    data['Distance roundtrip/Tx'] = pd.to_numeric(\n",
    "        data['Distance roundtrip/Tx'].astype(str).str.extract(r'(\\d+\\.?\\d*)')[0],\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Clean Referral Source\n",
    "    data['Referral Source'] = data['Referral Source'].astype(str).str.strip().replace(r'^\\s*$', 'Missing', regex=True)\n",
    "\n",
    "    # Clean Payment Method\n",
    "    data['Payment Method'] = data['Payment Method'].astype(str).str.strip().replace(r'^\\s*$', 'Missing', regex=True).replace('nan', 'Missing')\n",
    "     # Strip everything except letters\n",
    "    data['Payment Method'] = data['Payment Method'].str.replace(r'[^a-zA-Z\\s]','', regex=True).str.strip()\n",
    "    #Uppercase\n",
    "    data['Payment Method'] = data['Payment Method'].str.upper()\n",
    "\n",
    "    # Clean Remaining Balance\n",
    "    data['Remaining Balance'] = pd.to_numeric(data['Remaining Balance'], errors='coerce').round(2)\n",
    "    #Clean Amount\n",
    "    data['Amount'] = pd.to_numeric(data['Amount'], errors='coerce').round(2)\n",
    "\n",
    "    # Clean Patient Letter Notified\n",
    "    def letter_notified(val):\n",
    "        val = str(val).strip().lower()\n",
    "        if val in ['na', 'n/a', 'missing', '', 'nan']:\n",
    "            return 'No'\n",
    "        try:\n",
    "            pd.to_datetime(val)\n",
    "            return 'Yes'\n",
    "        except:\n",
    "            return 'No'\n",
    "    \n",
    "    #create date notified\n",
    "    data['Date Notified'] = data['Patient Letter Notified? (Directly/Indirectly through rep)']\n",
    "    # Convert to datetime and fill non-dates with NaT\n",
    "    data['Date Notified'] = pd.to_datetime(data['Date Notified'], errors='coerce')\n",
    "    #cleaning Patiend Letter Notified\n",
    "    data['Patient Letter Notified? (Directly/Indirectly through rep)'] = data['Patient Letter Notified? (Directly/Indirectly through rep)'].apply(letter_notified)\n",
    "       \n",
    "    #Clean Payable to:\n",
    "    # Replace blanks or whitespace-only strings with \"Missing\"\n",
    "    data['Payable to:'] = data['Payable to:'].replace(r'^\\s*$', \"Missing\", regex=True)\n",
    "\n",
    "    #Clean Application Signed?\n",
    "    data['Application Signed?'] = data['Application Signed?'].replace(np.nan, \"Missing\", regex=True)\n",
    "    data['Application Signed?'] = data['Application Signed?'].str.upper()\n",
    "\n",
    "    # Clean Type of Assistance\n",
    "    data['Type of Assistance (CLASS)'] = data['Type of Assistance (CLASS)'].astype(str).str.strip().str.lower()\n",
    "    data['Type of Assistance (CLASS)'] = data['Type of Assistance (CLASS)'].apply(\n",
    "        lambda x: 'utilities' if x.startswith('u') else x\n",
    "    )\n",
    "    data['Type of Assistance (CLASS)'] = data['Type of Assistance (CLASS)'].str.title()\n",
    "\n",
    "    # Clean Marital Status\n",
    "    data['Marital Status'] = data['Marital Status'].astype(str).str.strip().str.lower().str.title()\n",
    "    data['Marital Status'] = data['Marital Status'].apply(lambda x: 'Seperated' if x.startswith('Se') else x)\n",
    "\n",
    "    # Clean Gender\n",
    "    data['Gender'] = data['Gender'].astype(str).str.strip().str.lower().str.title()\n",
    "\n",
    "    # Export cleaned data for testing\n",
    "    output_path = r\"C:\\Users\\Glen\\Documents\\ToolsForDataAnalysis\\SemesterProject\\cleaned_data.csv\"\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "    #Export cleaned data as part of github action\n",
    "    #output_path = os.path.join(\"output\", \"cleaned_data.csv\")\n",
    "    #os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    #data.to_csv(output_path, index=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-3>:65: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "<positron-console-cell-3>:88: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "<positron-console-cell-3>:184: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient ID#</th>\n",
       "      <th>Grant Req Date</th>\n",
       "      <th>App Year</th>\n",
       "      <th>Remaining Balance</th>\n",
       "      <th>Request Status</th>\n",
       "      <th>Payment Submitted?</th>\n",
       "      <th>Reason - Pending/No</th>\n",
       "      <th>Pt City</th>\n",
       "      <th>Pt State</th>\n",
       "      <th>Pt Zip</th>\n",
       "      <th>Language</th>\n",
       "      <th>DOB</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Hispanic/Latino</th>\n",
       "      <th>Sexual Orientation</th>\n",
       "      <th>Insurance Type</th>\n",
       "      <th>Household Size</th>\n",
       "      <th>Total Household Gross Monthly Income</th>\n",
       "      <th>Distance roundtrip/Tx</th>\n",
       "      <th>Referral Source</th>\n",
       "      <th>Referred By:</th>\n",
       "      <th>Type of Assistance (CLASS)</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Payable to:</th>\n",
       "      <th>Patient Letter Notified? (Directly/Indirectly through rep)</th>\n",
       "      <th>Application Signed?</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Payment Submitted Date</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Date Notified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180001</td>\n",
       "      <td>10/17/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1180.00</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>Dr. Natarajan/Lily Salinas</td>\n",
       "      <td>Medical Supplies/Prescription Co-Pay(S)</td>\n",
       "      <td>320.00</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>190001</td>\n",
       "      <td>01/03/2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1428.39</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>Pam Owen/Sheri Shannon\\n</td>\n",
       "      <td>Medical Supplies/Prescription Co-Pay(S)</td>\n",
       "      <td>21.61</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190001</td>\n",
       "      <td>03/11/2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1428.39</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>Teresa Pfister</td>\n",
       "      <td>Food/Groceries</td>\n",
       "      <td>50.00</td>\n",
       "      <td>GC</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190002</td>\n",
       "      <td>05/20/2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1400.00</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>AG/Susan Keith</td>\n",
       "      <td>Food/Groceries</td>\n",
       "      <td>100.00</td>\n",
       "      <td>GC</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190003</td>\n",
       "      <td>05/22/2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1425.00</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NCS</td>\n",
       "      <td>AG/Kristi McHugh</td>\n",
       "      <td>Medical Supplies/Prescription Co-Pay(S)</td>\n",
       "      <td>75.00</td>\n",
       "      <td>CC</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>240393</td>\n",
       "      <td>01/31/2025</td>\n",
       "      <td>2</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>No</td>\n",
       "      <td>HS</td>\n",
       "      <td>Falls City</td>\n",
       "      <td>NE</td>\n",
       "      <td>68355</td>\n",
       "      <td>English</td>\n",
       "      <td>1960-09-23</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>CPN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gas</td>\n",
       "      <td>500.00</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>Waiting on HS</td>\n",
       "      <td>NaT</td>\n",
       "      <td>40.0742</td>\n",
       "      <td>-95.5931</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>240393</td>\n",
       "      <td>01/31/2025</td>\n",
       "      <td>2</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>No</td>\n",
       "      <td>HS</td>\n",
       "      <td>Falls City</td>\n",
       "      <td>NE</td>\n",
       "      <td>68355</td>\n",
       "      <td>English</td>\n",
       "      <td>1960-09-23</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Non-Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>CPN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Food/Groceries</td>\n",
       "      <td>500.00</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>Waiting on HS</td>\n",
       "      <td>NaT</td>\n",
       "      <td>40.0742</td>\n",
       "      <td>-95.5931</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>240548</td>\n",
       "      <td>01/31/2025</td>\n",
       "      <td>2</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>NE</td>\n",
       "      <td>68025</td>\n",
       "      <td>English</td>\n",
       "      <td>1962-04-03</td>\n",
       "      <td>Married</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Private</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2895.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NCS</td>\n",
       "      <td>ALISA SEIDLER</td>\n",
       "      <td>Multiple</td>\n",
       "      <td>1068.56</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>41.4416</td>\n",
       "      <td>-96.4945</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>250038</td>\n",
       "      <td>01/31/2025</td>\n",
       "      <td>1</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hastings</td>\n",
       "      <td>NE</td>\n",
       "      <td>68901</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1980-10-02</td>\n",
       "      <td>Single</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>2.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Morrison Cancer Center</td>\n",
       "      <td>Kellie Sterkel-SW</td>\n",
       "      <td>Housing</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>40.5877</td>\n",
       "      <td>-98.3911</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>250040</td>\n",
       "      <td>01/31/2025</td>\n",
       "      <td>1</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>Pending</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hastings</td>\n",
       "      <td>NE</td>\n",
       "      <td>68901</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1980-10-02</td>\n",
       "      <td>Single</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Straight</td>\n",
       "      <td>Uninsured</td>\n",
       "      <td>2.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Morrison Cancer Center</td>\n",
       "      <td>Kellie Sterkel</td>\n",
       "      <td>Housing</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>40.5877</td>\n",
       "      <td>-98.3911</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2292 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Patient ID# Grant Req Date  App Year  Remaining Balance  ... Payment Submitted Date Latitude Longitude Date Notified\n",
       "0          180001     10/17/2018         1            1180.00  ...                    NaT      NaN       NaN           NaT\n",
       "1          190001     01/03/2019         1            1428.39  ...                    NaT      NaN       NaN           NaT\n",
       "2          190001     03/11/2019         1            1428.39  ...                    NaT      NaN       NaN           NaT\n",
       "3          190002     05/20/2019         1            1400.00  ...                    NaT      NaN       NaN           NaT\n",
       "4          190003     05/22/2019         1            1425.00  ...                    NaT      NaN       NaN           NaT\n",
       "...           ...            ...       ...                ...  ...                    ...      ...       ...           ...\n",
       "2287       240393     01/31/2025         2            1000.00  ...                    NaT  40.0742  -95.5931           NaT\n",
       "2288       240393     01/31/2025         2            1000.00  ...                    NaT  40.0742  -95.5931           NaT\n",
       "2289       240548     01/31/2025         2            1000.00  ...                    NaT  41.4416  -96.4945           NaT\n",
       "2290       250038     01/31/2025         1            1500.00  ...                    NaT  40.5877  -98.3911           NaT\n",
       "2291       250040     01/31/2025         1            1500.00  ...                    NaT  40.5877  -98.3911           NaT\n",
       "\n",
       "[2292 rows x 34 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing function\n",
    "clean_data(\"C:\\\\Users\\\\Glen\\\\Documents\\\\ToolsForDataAnalysis\\\\SemesterProject\\\\Raw Data and Dict\\\\UNO Service Learning Data Sheet De-Identified Version.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        raise ValueError(\"No input file provided. Usage: python clean_data_script.py <input_file>\")\n",
    "\n",
    "    input_file = sys.argv[1]\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file '{input_file}' not found.\")\n",
    "\n",
    "    # Determine output file\n",
    "    output_file = os.path.splitext(input_file)[0] + \"_CLEANED.csv\"\n",
    "    sheet_name = \"PA Log Sheet\" if input_file.endswith(\".xlsx\") else None\n",
    "\n",
    "    print(f\"Reading from: {input_file}\")\n",
    "    cleaned_df = clean_data(input_file, sheet_name=sheet_name)\n",
    "\n",
    "    print(f\"Saving cleaned data to: {output_file}\")\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Cleaning completed: {input_file} -> {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub action steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name: Clean New Data\n",
    "\n",
    "permissions:\n",
    "  contents: write\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ \"main\" ]\n",
    "    paths:\n",
    "      - '**/*.csv'\n",
    "      - '**/*.xlsx'\n",
    "\n",
    "jobs:\n",
    "  clean-data:\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "    - name: Checkout code\n",
    "      uses: actions/checkout@v4\n",
    "      with:\n",
    "        fetch-depth: 2\n",
    "\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v5\n",
    "      with:\n",
    "        python-version: '3.x'\n",
    "\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install pandas numpy requests openpyxl\n",
    "\n",
    "    - name: Detect changed file\n",
    "      id: detect_file\n",
    "      run: |\n",
    "        changed_file=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep -E '\\.csv$|\\.xlsx$' | head -n 1 || true)\n",
    "        echo \"Changed file: $changed_file\"\n",
    "        echo \"CHANGED_FILE=$changed_file\" >> $GITHUB_ENV\n",
    "  \n",
    "    - name: Run data cleaning script\n",
    "      if: env.CHANGED_FILE != ''\n",
    "      run: |\n",
    "        echo \"Cleaning file: ${{ env.CHANGED_FILE }}\"\n",
    "        python clean_data_script.py \"${{ env.CHANGED_FILE }}\"\n",
    "\n",
    "    - name: Commit cleaned data\n",
    "      run: |\n",
    "        git config --global user.name 'github-actions[bot]'\n",
    "        git config --global user.email 'github-actions[bot]@users.noreply.github.com'\n",
    "        git add *_CLEANED.csv\n",
    "        git commit -m \"Automated: Cleaned $CHANGED_FILE\"\n",
    "        git push\n",
    "      continue-on-error: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import glob\n",
    "import os\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "#for use in github\n",
    "# Find all CSV files ending in _CLEANED.csv in the current directory\n",
    "#cleaned_files = glob.glob(\"*_CLEANED.csv\")\n",
    "#Print which files are being read\n",
    "#print(f\"Found cleaned files: {cleaned_files}\")\n",
    "# Combine them into a single DataFrame\n",
    "#df_list = [pd.read_csv(f) for f in cleaned_files]\n",
    "#db_data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "#load data\n",
    "db_data = pd.read_csv(\"C:\\\\Users\\\\Glen\\\\Documents\\\\ToolsForDataAnalysis\\\\SemesterProject\\\\cleaned_data.csv\")\n",
    "#converting grant req date to datetime\n",
    "db_data[\"Grant Req Date\"] = pd.to_datetime(db_data[\"Grant Req Date\"])\n",
    "#creating new column for assitance duration\n",
    "\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title = \"Nebraska Cancer Specialists Hope Foundation Dashboard\",\n",
    "    layout = \"wide\")\n",
    "st.title(\"Nebraska Cancer Specialists Hope Foundation Dashboard\")\n",
    "\n",
    "st.logo(image=\"https://www.bricksrus.com/donorsite/images/logo-NCSHF.png\", \n",
    "        icon_image=\"https://ncshopefoundation.org/wp-content/uploads/2023/05/sun.webp\")\n",
    "\n",
    "#Page navigation\n",
    "with st.sidebar:\n",
    "     page = st.radio(\"Select a Page\", [\"Pending Applications\", \"Assistance Given by Demographics\", \"Assistance Delivery Duration\", \"Grant Utilization\", \"Executive Impact Summary\"])\n",
    "     max_date = db_data['Grant Req Date'].max().date()\n",
    "     min_date = db_data['Grant Req Date'].min().date()\n",
    "     default_start_date = min_date  # Show all time by default\n",
    "     default_end_date = max_date\n",
    "     start_date = st.date_input(\"Start date\", default_start_date, min_value=db_data['Grant Req Date'].min().date(), max_value=max_date)\n",
    "     end_date = st.date_input(\"End date\", default_end_date, min_value=db_data['Grant Req Date'].min().date(), max_value=max_date)\n",
    "\n",
    "#filtering db data based on user selection\n",
    "db_data = db_data[(db_data['Grant Req Date'].dt.date >= start_date) & (db_data['Grant Req Date'].dt.date <= end_date)]\n",
    "\n",
    "# Pending Applications Page - page 1\n",
    "if page == \"Pending Applications\":\n",
    "    st.header(\"Pending Applications\")\n",
    "    # Filter by \"Application Signed?\"\n",
    "    signature_options = [\"All\"] + list(pd.unique(db_data[\"Application Signed?\"]))\n",
    "    signed_filter = st.selectbox(\"Filter by Signature Status:\", signature_options)\n",
    "    if signed_filter != \"All\":\n",
    "        db_data = db_data[db_data[\"Application Signed?\"] == signed_filter]\n",
    "    \n",
    "    #page 1 dataframe\n",
    "    db_data_pending = db_data[db_data[\"Request Status\"] == \"Pending\"]\n",
    "    #create page 1 card dfs\n",
    "    pending_apps = db_data_pending[\"Patient ID#\"].nunique()\n",
    "    signed_apps = db_data_pending[db_data_pending[\"Application Signed?\"] == \"YES\"][\"Patient ID#\"].nunique()\n",
    "    missing_apps = db_data_pending[db_data_pending[\"Application Signed?\"] == \"MISSING\"][\"Patient ID#\"].nunique()\n",
    "    unsigned_apps = db_data_pending[db_data_pending[\"Application Signed?\"] == \"NO\"][\"Patient ID#\"].nunique()\n",
    "    total_amt_pending = db_data_pending[\"Amount\"].sum().round()\n",
    "    #create page 1 cards\n",
    "    kpi1, kpi2, kpi3, kpi4, kpi5 = st.columns(5)\n",
    "    kpi1.metric(label = \"Pending Applications\", value = pending_apps)\n",
    "    kpi2.metric(label = \"Signed Pending Applications\", value = signed_apps)\n",
    "    kpi3.metric(label = \"Pending Unsigned Applications\", value = unsigned_apps)\n",
    "    kpi4.metric(label = \"Pending Applications Missing Signature Status\", value = missing_apps)\n",
    "    kpi5.metric(label = \"Pending Amount Requested\", value = f\"${total_amt_pending}\")\n",
    "    \n",
    "    #Dataframe filtered to only pending applications\n",
    "    pg1_df = db_data[db_data[\"Request Status\"] == \"Pending\"][\n",
    "        [\"Patient ID#\", \"Grant Req Date\", \"Application Signed?\", \"Pt Zip\", \"Insurance Type\", \n",
    "         \"Total Household Gross Monthly Income\", \"Type of Assistance (CLASS)\", \"Amount\", \n",
    "         \"Referral Source\", \"Notes\"]]\n",
    "    \n",
    "    #grouping assitance type by amount\n",
    "    p1_bar_data = db_data_pending.groupby(\"Type of Assistance (CLASS)\")[\"Amount\"].sum().reset_index()\n",
    "    #streamlit barchart for assitance type by amount\n",
    "    p1_bar = st.bar_chart(data = p1_bar_data, x=\"Type of Assistance (CLASS)\", y=\"Amount\", x_label = \"Assistance Type\", y_label = \"Amount Requested\", horizontal = False)\n",
    "    #pending applications table\n",
    "    pg1 = st.data_editor(pg1_df)\n",
    "    \n",
    "#Assistance amount by demographic factors - page 2\n",
    "elif page == \"Assistance Given by Demographics\":\n",
    "    #page header\n",
    "    st.header(\"Assistance Given by Demographics\")\n",
    "    #list of demographic factors\n",
    "    demo = [\n",
    "        'Gender', 'State', 'Zip Code', 'Hispanic or Latino', 'Sexuality', 'Race', 'Insurance Type', 'Household Gross Monthly Income', 'Marital Status', 'Household Size', 'Age']\n",
    "\n",
    "    #demographics selectbox\n",
    "    demo_select = st.selectbox(\"Select Demographic\", demo)\n",
    "\n",
    "    # filter & display data based on the selected demographic\n",
    "    if demo_select == \"Gender\":\n",
    "        # sum assistance by gender\n",
    "        gender_assistance = db_data.groupby(\"Gender\")[\"Amount\"].sum()  \n",
    "        st.bar_chart(gender_assistance)\n",
    "        st.write(gender_assistance)\n",
    "\n",
    "    elif demo_select == \"Insurance Type\":\n",
    "        insurance_assistance = db_data.groupby(\"Insurance Type\")[\"Amount\"].sum()  \n",
    "        st.bar_chart(insurance_assistance)\n",
    "        st.write(insurance_assistance)\n",
    "\n",
    "    elif demo_select == \"Sexuality\":\n",
    "        sexuality_assistance = db_data.groupby(\"Sexual Orientation\")[\"Amount\"].sum()  \n",
    "        st.bar_chart(sexuality_assistance)\n",
    "        st.write(sexuality_assistance)\n",
    "\n",
    "    elif demo_select == \"Race\":\n",
    "        racial_assistance = db_data.groupby(\"Race\")[\"Amount\"].sum()\n",
    "        st.bar_chart(racial_assistance)\n",
    "        st.write(racial_assistance)\n",
    "\n",
    "    elif demo_select == \"Hispanic or Latino\":\n",
    "        ethnicity_assistance = db_data.groupby(\"Hispanic/Latino\")[\"Amount\"].sum()  \n",
    "        st.bar_chart(ethnicity_assistance)\n",
    "        st.write(ethnicity_assistance)\n",
    "\n",
    "    elif demo_select == 'State':\n",
    "        state_assistance = db_data.groupby(\"Pt State\")[\"Amount\"].sum()\n",
    "        st.bar_chart(state_assistance)\n",
    "        st.write(state_assistance)\n",
    "\n",
    "    elif demo_select == \"Zip Code\":\n",
    "        st.header(\"Assistance by Zip Code\")\n",
    "\n",
    "        # Aggregate the total Amount by Zip Code\n",
    "        zip_code_assistance = db_data.groupby(\"Pt Zip\")[\"Amount\"].sum()\n",
    "\n",
    "        #map data\n",
    "        map_data = db_data[['Latitude', 'Longitude', 'Amount', 'Pt Zip']]\n",
    "        # Ensure the 'Amount' column is numeric\n",
    "        map_data[\"Amount\"] = pd.to_numeric(map_data[\"Amount\"], errors=\"coerce\")\n",
    "\n",
    "        # Drop rows without valid zip codes or amount\n",
    "        map_data = map_data.dropna(subset=[\"Pt Zip\", \"Amount\", \"Latitude\", \"Longitude\"])\n",
    "\n",
    "        # Create a scatter plot map\n",
    "        fig = px.scatter_geo(\n",
    "            map_data, lat=\"Latitude\", lon=\"Longitude\", color=\"Amount\", hover_name=\"Pt Zip\", hover_data=[\"Amount\"], color_continuous_scale=\"Viridis\", projection=\"albers usa\", title=\"Assistance Amounts by Zip Code\",)\n",
    "\n",
    "        # Update map settings for better visualization\n",
    "        fig.update_geos(showcoastlines=True, coastlinecolor=\"Black\", showland=True, landcolor=\"lightgray\")\n",
    "        fig.update_layout(\n",
    "            geo=dict( projection_type=\"albers usa\", showland=True, landcolor=\"lightgray\", subunitcolor=\"gray\",),\n",
    "            title_text=\"Assistance Amounts by Zip Code\", coloraxis_colorbar_title=\"Assistance Amount\")\n",
    "        \n",
    "        # Display in Streamlit\n",
    "        st.plotly_chart(fig)\n",
    "        \n",
    "        # Show the assistance by Zip Code table\n",
    "        st.write(zip_code_assistance)\n",
    "\n",
    "    elif demo_select == \"Marital Status\":\n",
    "        marriage_assistance = db_data.groupby('Marital Status')['Amount'].sum()\n",
    "        st.bar_chart(marriage_assistance)\n",
    "        st.write(marriage_assistance)\n",
    "\n",
    "    elif demo_select == \"Household Size\":\n",
    "        householdsize_assistance = db_data.groupby('Household Size')['Amount'].sum()\n",
    "        st.bar_chart(householdsize_assistance)\n",
    "        st.write(householdsize_assistance) \n",
    "\n",
    "#Time to Support - page 3\n",
    "elif page == \"Assistance Delivery Duration\":\n",
    "    st.header(\"Assistance Delivery Duration\")\n",
    "    #convert request date and payment submitted date to datetime\n",
    "    db_data[\"Payment Submitted Date\"] = pd.to_datetime(db_data[\"Payment Submitted Date\"], errors='coerce')\n",
    "    db_data[\"Grant Req Date\"] = pd.to_datetime(db_data[\"Grant Req Date\"], errors='coerce')\n",
    "    #date difference to get days to assist\n",
    "    db_data[\"Time to Assistance\"] = (db_data[\"Payment Submitted Date\"] - db_data[\"Grant Req Date\"]).dt.days.round(2)\n",
    "    #convert results to numeric\n",
    "    db_data[\"Time to Assistance\"] = pd.to_numeric(db_data[\"Time to Assistance\"])\n",
    "    #card for average\n",
    "    kpi9_data = db_data[\"Time to Assistance\"].dropna().mean().round(2)\n",
    "    kpi9 = st.columns(1)\n",
    "    kpi9[0].metric(label = \"Average Assistance Delivery Duration\", value = kpi9_data)\n",
    "    #histogram of duration\n",
    "    st.bar_chart(db_data[\"Time to Assistance\"].value_counts().sort_index())\n",
    "\n",
    "elif page == \"Grant Utilization\":\n",
    "    ug_db = db_data[db_data[\"Remaining Balance\"]>0]\n",
    "    #number of underutilized grants\n",
    "    underutilized_grants = ug_db[\"Patient ID#\"].nunique()\n",
    "    #card for underutilized grants\n",
    "    kpi10 = st.columns(1)\n",
    "    kpi10[0].metric(label = \"Number of Underutlized Grants\", value = underutilized_grants)\n",
    "    #bar chart of underutilization by assistance type\n",
    "    ug_db_grouped = ug_db.groupby(\"Type of Assistance (CLASS)\")[\"Amount\"].sum().reset_index()\n",
    "    st.bar_chart(data = ug_db_grouped, x=\"Type of Assistance (CLASS)\", y=\"Amount\", x_label = \"Assistance Type\", y_label = \"Amount Requested\", horizontal = False)\n",
    "\n",
    "\n",
    "#Executive Impact Summary Page - page 5\n",
    "elif page == \"Executive Impact Summary\":\n",
    "    st.header(\"Exective Impact Summary\")\n",
    "    #page 4 dataframe\n",
    "    pg4_df = db_data[db_data[\"Request Status\"] == \"Approved\"]\n",
    "    pg4_df[\"City, State\"] = pg4_df[\"Pt City\"] + \" , \" + pg4_df[\"Pt State\"] .fillna('')\n",
    "    #page 4 card dataframes\n",
    "    total_amt_awarded_df= pg4_df[\"Amount\"].sum().round()\n",
    "    total_applicants_awarded_df = pg4_df[\"Patient ID#\"].nunique()\n",
    "    total_amount = pg4_df[\"Amount\"].sum()\n",
    "    num_unique_patients = pg4_df[\"Patient ID#\"].nunique()\n",
    "    avg_award = (total_amount / num_unique_patients).round(2)\n",
    "    #page 4 cards\n",
    "    kpi6, kpi7, kpi8 = st.columns(3)\n",
    "    kpi6.metric(label = \"Total Assistance Awarded\", value = f\"${total_amt_awarded_df}\")\n",
    "    kpi7.metric(label = \"Total Applicants Awarded\", value = total_applicants_awarded_df)\n",
    "    kpi8.metric(label = \"Average Assistance Amount per Patient\", value = f\"${avg_award}\")\n",
    "\n",
    "    #bar graph of contribution by insurance type\n",
    "    p4_bar1_data = pg4_df.groupby(\"Insurance Type\")[\"Amount\"].sum().reset_index()\n",
    "    st.subheader(\"Assistance Given by Insurance Type\")\n",
    "    p4_bar1 = st.bar_chart(data = p4_bar1_data, x = \"Insurance Type\", y = \"Amount\")\n",
    "\n",
    "    #average amount by assistance type\n",
    "    pg4_df_grouped_avg = pg4_df.groupby(\"Type of Assistance (CLASS)\")[\"Amount\"].mean().reset_index()\n",
    "    st.subheader(\"Average Assistance Amount by Type\")\n",
    "    st.bar_chart(data = pg4_df_grouped_avg, x = \"Type of Assistance (CLASS)\", y = \"Amount\", x_label = \"Assistance Type\", y_label = \"Amount Approved\")\n",
    "    \n",
    "    #bar graph of contribution by assistance type\n",
    "    p4_bar2_data = pg4_df.groupby(\"Type of Assistance (CLASS)\")[\"Amount\"].sum().reset_index()\n",
    "    st.subheader(\"Total Assistance by Type\")\n",
    "    p4_bar2 = st.bar_chart(data = p4_bar2_data, x = \"Type of Assistance (CLASS)\", y = \"Amount\", x_label = \"Assistance Type\", y_label = \"Amount Approved\" )\n",
    "    \n",
    "    #bar graph of contribution by top city, state\n",
    "    p4_bar3_data = pg4_df.groupby(\"City, State\")[\"Amount\"].sum().reset_index().sort_values(by = \"Amount\", ascending = False).head(20)\n",
    "    p4_bar3_data = p4_bar3_data.sort_values(by=\"Amount\", ascending=True)\n",
    "    st.subheader(\"Total Assistance by Top 20 City and State\")\n",
    "    p4_bar3 = st.bar_chart(data = p4_bar3_data, x = \"City, State\", y = \"Amount\")\n",
    "    \n",
    "    #bar graph of contribution by bottom city, state\n",
    "    p4_bar4_data = pg4_df.groupby(\"City, State\")[\"Amount\"].sum().reset_index().sort_values(by = \"Amount\", ascending = False).tail(20)\n",
    "    p4_bar4_data = p4_bar4_data.sort_values(by=\"Amount\", ascending=True)\n",
    "    st.subheader(\"Total Assistance by Bottom 20 City and State\")\n",
    "    p4_bar4 = st.bar_chart(data = p4_bar4_data, x = \"City, State\", y = \"Amount\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
